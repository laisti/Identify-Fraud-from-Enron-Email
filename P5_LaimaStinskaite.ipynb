{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P5: Identify Fraud from Enron Email\n",
    "\n",
    "*by Laima Stinskaite*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In 2000, Enron was one of the largest companies in the United States. By 2002, it had collapsed into bankruptcy due to widespread corporate fraud. In the resulting Federal investigation, there was a significant amount of typically confidential information entered into public record, including tens of thousands of emails and detailed financial data for top executives.\n",
    "\n",
    "In this document we try to identify POIs (person of interest) by using machine learning techniques. POI was who indicted, reached a settlement or plea deal with the government, or testified in exchange for prosecution immunity. \n",
    "\n",
    "So the goal of this project is to train and tune a machine learning algorithm that can predict which employees are POIs using Enron financial and email data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Enron email and financial data was combined into a dictionary, where each key-value pair in the dictionary corresponds to one person. The dictionary key is the person's name, and the value is another dictionary, which contains the names of all the features and their values for that person. The features in the data fall into three major types, namely financial features, email features and POI labels.\n",
    "\n",
    "__financial features__: ['salary', 'deferral_payments', 'total_payments', 'loan_advances', 'bonus', 'restricted_stock_deferred', 'deferred_income', 'total_stock_value', 'expenses', 'exercised_stock_options', 'other', 'long_term_incentive', 'restricted_stock', 'director_fees'] (all units are in US dollars)\n",
    "\n",
    "__email features__: ['to_messages', 'email_address', 'from_poi_to_this_person', 'from_messages', 'from_this_person_to_poi', 'shared_receipt_with_poi'] (units are generally number of emails messages; notable exception is ‘email_address’, which is a text string)\n",
    "\n",
    "__POI label__: [‘poi’] (boolean, represented as integer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Import all necessary in this project libraries\n",
    "import sys\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "sys.path.append(\"../tools/\")\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from time import time\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from tester import test_classifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Load the dictionary containing the dataset\n",
    "with open(\"final_project_dataset.pkl\", \"r\") as data_file:\n",
    "    data_dict = pickle.load(data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we load our data we will investigate it a little bit. We are interested in the following aspects:\n",
    "- number of rows (total number of people) in the dataset\n",
    "- names of all people in the dataset\n",
    "- number of features for every person\n",
    "- list of features for person\n",
    "- number of POIs\n",
    "- names of POIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of people in our dataset: 146\n",
      "\n",
      "['METTS MARK', 'BAXTER JOHN C', 'ELLIOTT STEVEN', 'CORDES WILLIAM R', 'HANNON KEVIN P', 'MORDAUNT KRISTINA M', 'MEYER ROCKFORD G', 'MCMAHON JEFFREY', 'HORTON STANLEY C', 'PIPER GREGORY F', 'HUMPHREY GENE E', 'UMANOFF ADAM S', 'BLACHMAN JEREMY M', 'SUNDE MARTIN', 'GIBBS DANA R', 'LOWRY CHARLES P', 'COLWELL WESLEY', 'MULLER MARK S', 'JACKSON CHARLENE R', 'WESTFAHL RICHARD K', 'WALTERS GARETH W', 'WALLS JR ROBERT H', 'KITCHEN LOUISE', 'CHAN RONNIE', 'BELFER ROBERT', 'SHANKMAN JEFFREY A', 'WODRASKA JOHN', 'BERGSIEKER RICHARD P', 'URQUHART JOHN A', 'BIBI PHILIPPE A', 'RIEKER PAULA H', 'WHALEY DAVID A', 'BECK SALLY W', 'HAUG DAVID L', 'ECHOLS JOHN B', 'MENDELSOHN JOHN', 'HICKERSON GARY J', 'CLINE KENNETH W', 'LEWIS RICHARD', 'HAYES ROBERT E', 'MCCARTY DANNY J', 'KOPPER MICHAEL J', 'LEFF DANIEL P', 'LAVORATO JOHN J', 'BERBERIAN DAVID', 'DETMERING TIMOTHY J', 'WAKEHAM JOHN', 'POWERS WILLIAM', 'GOLD JOSEPH', 'BANNANTINE JAMES M', 'DUNCAN JOHN H', 'SHAPIRO RICHARD S', 'SHERRIFF JOHN R', 'SHELBY REX', 'LEMAISTRE CHARLES', 'DEFFNER JOSEPH M', 'KISHKILL JOSEPH G', 'WHALLEY LAWRENCE G', 'MCCONNELL MICHAEL S', 'PIRO JIM', 'DELAINEY DAVID W', 'SULLIVAN-SHAKLOVITZ COLLEEN', 'WROBEL BRUCE', 'LINDHOLM TOD A', 'MEYER JEROME J', 'LAY KENNETH L', 'BUTTS ROBERT H', 'OLSON CINDY K', 'MCDONALD REBECCA', 'CUMBERLAND MICHAEL S', 'GAHN ROBERT S', 'MCCLELLAN GEORGE', 'HERMANN ROBERT J', 'SCRIMSHAW MATTHEW', 'GATHMANN WILLIAM D', 'HAEDICKE MARK E', 'BOWEN JR RAYMOND M', 'GILLIS JOHN', 'FITZGERALD JAY L', 'MORAN MICHAEL P', 'REDMOND BRIAN L', 'BAZELIDES PHILIP J', 'BELDEN TIMOTHY N', 'DURAN WILLIAM D', 'THORN TERENCE H', 'FASTOW ANDREW S', 'FOY JOE', 'CALGER CHRISTOPHER F', 'RICE KENNETH D', 'KAMINSKI WINCENTY J', 'LOCKHART EUGENE E', 'COX DAVID', 'OVERDYKE JR JERE C', 'PEREIRA PAULO V. FERRAZ', 'STABLER FRANK', 'SKILLING JEFFREY K', 'BLAKE JR. NORMAN P', 'SHERRICK JEFFREY B', 'PRENTICE JAMES', 'GRAY RODNEY', 'PICKERING MARK R', 'THE TRAVEL AGENCY IN THE PARK', 'NOLES JAMES L', 'KEAN STEVEN J', 'TOTAL', 'FOWLER PEGGY', 'WASAFF GEORGE', 'WHITE JR THOMAS E', 'CHRISTODOULOU DIOMEDES', 'ALLEN PHILLIP K', 'SHARP VICTORIA T', 'JAEDICKE ROBERT', 'WINOKUR JR. HERBERT S', 'BROWN MICHAEL', 'BADUM JAMES P', 'HUGHES JAMES A', 'REYNOLDS LAWRENCE', 'DIMICHELE RICHARD G', 'BHATNAGAR SANJAY', 'CARTER REBECCA C', 'BUCHANAN HAROLD G', 'YEAP SOON', 'MURRAY JULIA H', 'GARLAND C KEVIN', 'DODSON KEITH', 'YEAGER F SCOTT', 'HIRKO JOSEPH', 'DIETRICH JANET R', 'DERRICK JR. JAMES V', 'FREVERT MARK A', 'PAI LOU L', 'BAY FRANKLIN R', 'HAYSLETT RODERICK J', 'FUGH JOHN L', 'FALLON JAMES B', 'KOENIG MARK E', 'SAVAGE FRANK', 'IZZO LAWRENCE L', 'TILNEY ELIZABETH A', 'MARTIN AMANDA K', 'BUY RICHARD B', 'GRAMM WENDY L', 'CAUSEY RICHARD A', 'TAYLOR MITCHELL S', 'DONAHUE JR JEFFREY M', 'GLISAN JR BEN F']\n",
      "\n",
      "Number of features for person: 21\n",
      "\n",
      "['salary', 'to_messages', 'deferral_payments', 'total_payments', 'exercised_stock_options', 'bonus', 'restricted_stock', 'shared_receipt_with_poi', 'restricted_stock_deferred', 'total_stock_value', 'expenses', 'loan_advances', 'from_messages', 'other', 'from_this_person_to_poi', 'poi', 'director_fees', 'deferred_income', 'long_term_incentive', 'email_address', 'from_poi_to_this_person']\n",
      "\n",
      "Number of POIs: 18\n",
      "\n",
      "['HANNON KEVIN P', 'COLWELL WESLEY', 'RIEKER PAULA H', 'KOPPER MICHAEL J', 'SHELBY REX', 'DELAINEY DAVID W', 'LAY KENNETH L', 'BOWEN JR RAYMOND M', 'BELDEN TIMOTHY N', 'FASTOW ANDREW S', 'CALGER CHRISTOPHER F', 'RICE KENNETH D', 'SKILLING JEFFREY K', 'YEAGER F SCOTT', 'HIRKO JOSEPH', 'KOENIG MARK E', 'CAUSEY RICHARD A', 'GLISAN JR BEN F']\n"
     ]
    }
   ],
   "source": [
    "### Number of rows in the dataset\n",
    "print 'Total number of people in our dataset: %d' % len(data_dict)\n",
    "print ''\n",
    "\n",
    "### Names of all people from the dataset\n",
    "print data_dict.keys()\n",
    "print ''\n",
    "\n",
    "### Number of features for person\n",
    "print 'Number of features for person: %d' % len(data_dict[\"METTS MARK\"].keys())\n",
    "print ''\n",
    "\n",
    "### List of features for person\n",
    "print data_dict[\"METTS MARK\"].keys()\n",
    "print ''\n",
    "\n",
    "### Number of POIs\n",
    "count = 0\n",
    "pois = []\n",
    "for person in data_dict:\n",
    "    if data_dict[person]['poi'] == 1:\n",
    "        count = count + 1\n",
    "        pois.append(person)\n",
    "print 'Number of POIs: %d' % count\n",
    "print ''\n",
    "\n",
    "### Names of POIs\n",
    "print pois"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we begin selection of features, let's first check our dataset for outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'METTS MARK': {'salary': 365788, 'to_messages': 807, 'deferral_payments': 'NaN', 'total_payments': 1061827, 'exercised_stock_options': 'NaN', 'bonus': 600000, 'restricted_stock': 585062, 'shared_receipt_with_poi': 702, 'restricted_stock_deferred': 'NaN', 'total_stock_value': 585062, 'expenses': 94299, 'loan_advances': 'NaN', 'from_messages': 29, 'other': 1740, 'from_this_person_to_poi': 1, 'poi': False, 'director_fees': 'NaN', 'deferred_income': 'NaN', 'long_term_incentive': 'NaN', 'email_address': 'mark.metts@enron.com', 'from_poi_to_this_person': 38}, 'BAXTER JOHN C': {'salary': 267102, 'to_messages': 'NaN', 'deferral_payments': 1295738, 'total_payments': 5634343, 'exercised_stock_options': 6680544, 'bonus': 1200000, 'restricted_stock': 3942714, 'shared_receipt_with_poi': 'NaN', 'restricted_stock_deferred': 'NaN', 'total_stock_value': 10623258, 'expenses': 11200, 'loan_advances': 'NaN', 'from_messages': 'NaN', 'other': 2660303, 'from_this_person_to_poi': 'NaN', 'poi': False, 'director_fees': 'NaN', 'deferred_income': -1386055, 'long_term_incentive': 1586055, 'email_address': 'NaN', 'from_poi_to_this_person': 'NaN'}, 'ELLIOTT STEVEN': {'salary': 170941, 'to_messages': 'NaN', 'deferral_payments': 'NaN', 'total_payments': 211725, 'exercised_stock_options': 4890344, 'bonus': 350000, 'restricted_stock': 1788391, 'shared_receipt_with_poi': 'NaN', 'restricted_stock_deferred': 'NaN', 'total_stock_value': 6678735, 'expenses': 78552, 'loan_advances': 'NaN', 'from_messages': 'NaN', 'other': 12961, 'from_this_person_to_poi': 'NaN', 'poi': False, 'director_fees': 'NaN', 'deferred_income': -400729, 'long_term_incentive': 'NaN', 'email_address': 'steven.elliott@enron.com', 'from_poi_to_this_person': 'NaN'}, 'CORDES WILLIAM R': {'salary': 'NaN', 'to_messages': 764, 'deferral_payments': 'NaN', 'total_payments': 'NaN', 'exercised_stock_options': 651850, 'bonus': 'NaN', 'restricted_stock': 386335, 'shared_receipt_with_poi': 58, 'restricted_stock_deferred': 'NaN', 'total_stock_value': 1038185, 'expenses': 'NaN', 'loan_advances': 'NaN', 'from_messages': 12, 'other': 'NaN', 'from_this_person_to_poi': 0, 'poi': False, 'director_fees': 'NaN', 'deferred_income': 'NaN', 'long_term_incentive': 'NaN', 'email_address': 'bill.cordes@enron.com', 'from_poi_to_this_person': 10}, 'HANNON KEVIN P': {'salary': 243293, 'to_messages': 1045, 'deferral_payments': 'NaN', 'total_payments': 288682, 'exercised_stock_options': 5538001, 'bonus': 1500000, 'restricted_stock': 853064, 'shared_receipt_with_poi': 1035, 'restricted_stock_deferred': 'NaN', 'total_stock_value': 6391065, 'expenses': 34039, 'loan_advances': 'NaN', 'from_messages': 32, 'other': 11350, 'from_this_person_to_poi': 21, 'poi': True, 'director_fees': 'NaN', 'deferred_income': -3117011, 'long_term_incentive': 1617011, 'email_address': 'kevin.hannon@enron.com', 'from_poi_to_this_person': 32}}\n"
     ]
    }
   ],
   "source": [
    "### Show first 5 persons from dataset\n",
    "first_persons = {k: data_dict[k] for k in data_dict.keys()[:5]}\n",
    "print first_persons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see, practically every person has NaN values. In this case outlier can be if person has NaN values for every feature. Let's check that our data doesn't consist of such people:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOCKHART EUGENE E\n"
     ]
    }
   ],
   "source": [
    "### Check if our dataset consists of people with all NaN features\n",
    "for person in data_dict:\n",
    "    if data_dict[person]['to_messages'] == 'NaN' and \\\n",
    "        data_dict[person]['shared_receipt_with_poi'] == 'NaN' and \\\n",
    "        data_dict[person]['from_messages'] == 'NaN' and \\\n",
    "        data_dict[person]['from_this_person_to_poi'] == 'NaN' and \\\n",
    "        data_dict[person]['email_address'] == 'NaN' and \\\n",
    "        data_dict[person]['from_poi_to_this_person'] == 'NaN' and \\\n",
    "        data_dict[person]['salary'] == 'NaN' and \\\n",
    "        data_dict[person]['deferral_payments'] == 'NaN' and \\\n",
    "        data_dict[person]['total_payments'] == 'NaN' and \\\n",
    "        data_dict[person]['exercised_stock_options'] == 'NaN' and \\\n",
    "        data_dict[person]['bonus'] == 'NaN' and \\\n",
    "        data_dict[person]['restricted_stock'] == 'NaN' and \\\n",
    "        data_dict[person]['restricted_stock_deferred'] == 'NaN' and \\\n",
    "        data_dict[person]['total_stock_value'] == 'NaN' and \\\n",
    "        data_dict[person]['expenses'] == 'NaN' and \\\n",
    "        data_dict[person]['loan_advances'] == 'NaN' and \\\n",
    "        data_dict[person]['other'] == 'NaN' and \\\n",
    "        data_dict[person]['director_fees'] == 'NaN' and \\\n",
    "        data_dict[person]['deferred_income'] == 'NaN' and \\\n",
    "        data_dict[person]['long_term_incentive'] == 'NaN':\n",
    "        print person"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, as we see, one person from our dataset has NaN values for every feature. It is definitely an outlier and we should remove it.\n",
    "\n",
    "Also, from course lesson we found 'TOTAL' key as an outlier. Let's take a look at relationship between salary and bonus one more time, to be sure that we have an outlier there:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAERCAYAAACU1LsdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSVLBUJAkFQwFSVLBUJAkFQwFSVLh/wP1B+q3c2iFtAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x9a9d5f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Create a plot for salary and bonus features\n",
    "features = [\"salary\", \"bonus\"]\n",
    "data = featureFormat(data_dict, features)\n",
    "\n",
    "for item in data:\n",
    "    salary = item[0]\n",
    "    bonus = item[1]\n",
    "    plt.scatter(salary, bonus)\n",
    "\n",
    "plt.xlabel(\"salary\")\n",
    "plt.ylabel(\"bonus\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, 'TOTAL' is an outlier and we should remove it from our dataset too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of people in our dataset: 144\n"
     ]
    }
   ],
   "source": [
    "### Remove LOCKHART EUGENE E from our dataset\n",
    "data_dict.pop('LOCKHART EUGENE E', 0)\n",
    "\n",
    "### Remove TOTAL from our dataset\n",
    "data_dict.pop('TOTAL', 0)\n",
    "\n",
    "### Show number of people in the dataset\n",
    "print 'Total number of people in our dataset: %d' % len(data_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every feature beside 'email_address' consists of numbers. 'email_address' feature consists of string value. As we know, we will perform different machine learning technics (work with numbers) further, so we should remove this feature from our features_list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features for person: 20\n"
     ]
    }
   ],
   "source": [
    "### Remove 'email_address' column\n",
    "for key in data_dict:\n",
    "    features_dict = data_dict[key]\n",
    "    features_dict.pop('email_address', 0)\n",
    "\n",
    "### Show number of features\n",
    "print 'Number of features for person: %d' % len(data_dict[\"METTS MARK\"].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's change all NaN values to 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'METTS MARK': {'salary': 365788, 'to_messages': 807, 'deferral_payments': 0, 'total_payments': 1061827, 'exercised_stock_options': 0, 'bonus': 600000, 'restricted_stock': 585062, 'shared_receipt_with_poi': 702, 'restricted_stock_deferred': 0, 'total_stock_value': 585062, 'expenses': 94299, 'loan_advances': 0, 'from_messages': 29, 'other': 1740, 'from_this_person_to_poi': 1, 'poi': False, 'director_fees': 0, 'deferred_income': 0, 'long_term_incentive': 0, 'from_poi_to_this_person': 38}, 'BAXTER JOHN C': {'salary': 267102, 'to_messages': 0, 'deferral_payments': 1295738, 'total_payments': 5634343, 'exercised_stock_options': 6680544, 'bonus': 1200000, 'restricted_stock': 3942714, 'shared_receipt_with_poi': 0, 'restricted_stock_deferred': 0, 'total_stock_value': 10623258, 'expenses': 11200, 'loan_advances': 0, 'from_messages': 0, 'other': 2660303, 'from_this_person_to_poi': 0, 'poi': False, 'director_fees': 0, 'deferred_income': -1386055, 'long_term_incentive': 1586055, 'from_poi_to_this_person': 0}, 'ELLIOTT STEVEN': {'salary': 170941, 'to_messages': 0, 'deferral_payments': 0, 'total_payments': 211725, 'exercised_stock_options': 4890344, 'bonus': 350000, 'restricted_stock': 1788391, 'shared_receipt_with_poi': 0, 'restricted_stock_deferred': 0, 'total_stock_value': 6678735, 'expenses': 78552, 'loan_advances': 0, 'from_messages': 0, 'other': 12961, 'from_this_person_to_poi': 0, 'poi': False, 'director_fees': 0, 'deferred_income': -400729, 'long_term_incentive': 0, 'from_poi_to_this_person': 0}, 'CORDES WILLIAM R': {'salary': 0, 'to_messages': 764, 'deferral_payments': 0, 'total_payments': 0, 'exercised_stock_options': 651850, 'bonus': 0, 'restricted_stock': 386335, 'shared_receipt_with_poi': 58, 'restricted_stock_deferred': 0, 'total_stock_value': 1038185, 'expenses': 0, 'loan_advances': 0, 'from_messages': 12, 'other': 0, 'from_this_person_to_poi': 0, 'poi': False, 'director_fees': 0, 'deferred_income': 0, 'long_term_incentive': 0, 'from_poi_to_this_person': 10}, 'HANNON KEVIN P': {'salary': 243293, 'to_messages': 1045, 'deferral_payments': 0, 'total_payments': 288682, 'exercised_stock_options': 5538001, 'bonus': 1500000, 'restricted_stock': 853064, 'shared_receipt_with_poi': 1035, 'restricted_stock_deferred': 0, 'total_stock_value': 6391065, 'expenses': 34039, 'loan_advances': 0, 'from_messages': 32, 'other': 11350, 'from_this_person_to_poi': 21, 'poi': True, 'director_fees': 0, 'deferred_income': -3117011, 'long_term_incentive': 1617011, 'from_poi_to_this_person': 32}}\n"
     ]
    }
   ],
   "source": [
    "### Change missing data by setting their value to 0\n",
    "for key in data_dict:\n",
    "    features_dict = data_dict[key]\n",
    "    for feature in features_dict:\n",
    "        if features_dict[feature] == 'NaN':\n",
    "            features_dict[feature] = 0\n",
    "\n",
    "### Show first 5 persons from dataset\n",
    "first_persons = {k: data_dict[k] for k in data_dict.keys()[:5]}\n",
    "print first_persons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we cleaned our dataset, we are ready to begin the feature creation and selection processes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance of all original features from dataset\n",
    "\n",
    "The next step in our analysis will be selecting features for machine learning algorithms. \n",
    "First, let's take a look at accuracy score of all features list using DecisionTreeClassifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def testClassifier(classifier):\n",
    "    \"\"\"\n",
    "    Function uses classifier and prints all its metrics\n",
    "    Arguments: classifier\n",
    "    \"\"\"\n",
    "    clf = classifier\n",
    "    t0 = time()\n",
    "    clf = clf.fit(features_train, labels_train)\n",
    "    print 'Training time: ', t0\n",
    "    t1 = time()\n",
    "    pred = clf.predict(features_test)\n",
    "    print 'Testing time: ', t1\n",
    "\n",
    "    ### metrics\n",
    "    features_accuracy = accuracy_score(labels_test, pred)\n",
    "    features_precision_score = precision_score(labels_test, pred)\n",
    "    features_recall_score = recall_score(labels_test, pred)\n",
    "    features_f1_score = f1_score(labels_test, pred)\n",
    "\n",
    "    print 'Accuracy: ', features_accuracy\n",
    "    print 'Precision: ', features_precision_score\n",
    "    print 'Recall: ', features_recall_score\n",
    "    print 'F1 score:', features_f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Classifier for original features list:\n",
      "Training time:  1494759539.21\n",
      "Testing time:  1494759539.21\n",
      "Accuracy:  0.840909090909\n",
      "Precision:  0.25\n",
      "Recall:  0.2\n",
      "F1 score: 0.222222222222\n"
     ]
    }
   ],
   "source": [
    "### List of original features\n",
    "features_list_original = ['poi', 'salary', 'to_messages', 'deferral_payments', 'total_payments', \\\n",
    "                'exercised_stock_options', 'bonus', 'restricted_stock', 'shared_receipt_with_poi', \\\n",
    "                'restricted_stock_deferred', 'total_stock_value', 'expenses', 'loan_advances', \\\n",
    "                'from_messages', 'other', 'from_this_person_to_poi', 'director_fees', \\\n",
    "                'deferred_income', 'long_term_incentive', 'from_poi_to_this_person']\n",
    "\n",
    "### Store to my_dataset for easy export below.\n",
    "my_dataset = data_dict\n",
    "\n",
    "### Extract features and labels from dataset for local testing\n",
    "data = featureFormat(my_dataset, features_list_original, sort_keys = True)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "\n",
    "### Split data into training and testing datasets\n",
    "features_train, features_test, labels_train, labels_test = train_test_split(features, labels, test_size = 0.3, random_state = 42)\n",
    "\n",
    "### Use DecisionTreeClassifier for all original features\n",
    "print 'Decision Tree Classifier for original features list:'\n",
    "testClassifier(DecisionTreeClassifier(random_state=42))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New feature creation\n",
    "\n",
    "Scaling the features is important to many machine learning algorithms because \"since the range of values of raw data varies widely, in some machine learning algorithms, objective functions will not work properly without normalization\". Also \"the gradient descent converges much faster with feature scaling than without it\". (https://en.wikipedia.org/wiki/Feature_scaling)\n",
    "\n",
    "Now let's take a look at 'to_messages', 'from_poi_to_this_person', 'from_messages', 'from_this_person_to_poi' features. They are all absolute statistics and not good enough to make comparison of the person's emails with POIs.\n",
    "\n",
    "Let's create 2 additional features for our features list:\n",
    "- ratio_to_poi (the ratio of emails to POIs to all emails sent)\n",
    "- ratio_from_poi (the ratio of emails from POIs to all emails received)\n",
    "\n",
    "We included these features in the dataset because they are relative measurements and they normalize emails data across the rest of the employees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features:  22\n"
     ]
    }
   ],
   "source": [
    "def computeRatio(poi_messages, all_messages):\n",
    "    \"\"\"\n",
    "    Function computes the ratio of messages sent and received from a POI\n",
    "    Arguments: poi_messages, all_messages\n",
    "    Return: ratio of arguments\n",
    "    \"\"\"\n",
    "    if all_messages != 0:\n",
    "        ratio = float(poi_messages) / float(all_messages)\n",
    "    else:\n",
    "        ratio = 0\n",
    "    return ratio\n",
    "\n",
    "### Create 2 new features: \"ratio_to_poi\" and \"ratio_from_poi\"\n",
    "for key in my_dataset:\n",
    "    features_dict = my_dataset[key]\n",
    "    ratio_to_poi = computeRatio(features_dict[\"from_this_person_to_poi\"], features_dict[\"from_messages\"])\n",
    "    features_dict[\"ratio_to_poi\"] = ratio_to_poi\n",
    "    ratio_from_poi = computeRatio(features_dict[\"from_poi_to_this_person\"], features_dict[\"to_messages\"])\n",
    "    features_dict[\"ratio_from_poi\"] = ratio_from_poi\n",
    "    \n",
    "print 'Number of features: ', len(features_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check what performance will be after we added new features to our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Classifier for all original features and new features:\n",
      "Training time:  1494759591.33\n",
      "Testing time:  1494759591.33\n",
      "Accuracy:  0.818181818182\n",
      "Precision:  0.2\n",
      "Recall:  0.2\n",
      "F1 score: 0.2\n"
     ]
    }
   ],
   "source": [
    "### List of original features + new features\n",
    "features_list_plus_new = ['poi', 'salary', 'to_messages', 'deferral_payments', 'total_payments', \\\n",
    "                'exercised_stock_options', 'bonus', 'restricted_stock', 'shared_receipt_with_poi', \\\n",
    "                'restricted_stock_deferred', 'total_stock_value', 'expenses', 'loan_advances', \\\n",
    "                'from_messages', 'other', 'from_this_person_to_poi', 'director_fees', \\\n",
    "                'deferred_income', 'long_term_incentive', 'from_poi_to_this_person', \\\n",
    "                'ratio_to_poi', 'ratio_from_poi']\n",
    "\n",
    "### Extract features and labels from dataset for local testing\n",
    "data = featureFormat(my_dataset, features_list_plus_new, sort_keys = True)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "\n",
    "### Split data into training and testing datasets\n",
    "features_train, features_test, labels_train, labels_test = train_test_split(features, labels, test_size = 0.3, random_state = 42)\n",
    "\n",
    "### Use DecisionTreeClassifier for all original features and new features\n",
    "print 'Decision Tree Classifier for all original features and new features:'\n",
    "testClassifier(DecisionTreeClassifier(random_state=42))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see precision after adding new features became lower (0.2) than before adding them (0.25), so we won't use these features in our analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection using SelectKBest\n",
    "\n",
    "Now let's compare features scores using SelectKBest method with k = 'all'. We will use SelectKBest method with default value f-classif for score_func parameter.  I decided not to use chi2 scoring because there are many negative values in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features scores:  [inf, 772.43341185601332, 556.77730806873853, 287.2664203370756, 253.70964054494021, 73.48004838395714, 52.561787970591261, 43.569827232179648, 24.176713973334053, 3.6436367678039865, 3.3799033334556121, 3.0881544960334821, 2.7152382606791057, 2.4765688698248112, 1.712424636015998, 1.3179737674187317, 0.12814399385851574, 0.017226936204361963]\n"
     ]
    }
   ],
   "source": [
    "### Show features scores using SelectKBest\n",
    "slc = SelectKBest(k='all')\n",
    "slc = slc.fit(features, labels)\n",
    "scores = slc.scores_\n",
    "    \n",
    "print 'Features scores: ', sorted(scores, reverse = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see above, features scores drop off after 9th feature, so k = 9 features were chosen for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best features using SelectKBest:  ['poi', 'deferral_payments', 'exercised_stock_options', 'expenses', 'loan_advances', 'from_messages', 'other', 'director_fees', 'deferred_income', 'long_term_incentive']\n"
     ]
    }
   ],
   "source": [
    "### List of original features beside 'poi'\n",
    "features_list_without_poi = ['salary', 'to_messages', 'deferral_payments', 'total_payments', \\\n",
    "                'exercised_stock_options', 'bonus', 'restricted_stock', 'shared_receipt_with_poi', \\\n",
    "                'restricted_stock_deferred', 'total_stock_value', 'expenses', 'loan_advances', \\\n",
    "                'from_messages', 'other', 'from_this_person_to_poi', 'director_fees', \\\n",
    "                'deferred_income', 'long_term_incentive', 'from_poi_to_this_person']\n",
    "\n",
    "### Extract features and labels from dataset for local testing\n",
    "data = featureFormat(my_dataset, features_list_without_poi, sort_keys = True)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "\n",
    "### Use SelectKBest classifier for selection of 9 best features from our dataset\n",
    "slc = SelectKBest(k=9)\n",
    "slc = slc.fit(features, labels)\n",
    "features_list = [features_list_without_poi[i] for i in slc.get_support(indices=True)]\n",
    "\n",
    "### Insert 'poi' to the best_features list\n",
    "features_list.insert(0, 'poi') \n",
    "\n",
    "print 'Best features using SelectKBest: ', features_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Classifier for best 9 features and \"poi\":\n",
      "Training time:  1494759642.51\n",
      "Testing time:  1494759642.51\n",
      "Accuracy:  0.860465116279\n",
      "Precision:  0.5\n",
      "Recall:  0.5\n",
      "F1 score: 0.5\n"
     ]
    }
   ],
   "source": [
    "### Extract features and labels from dataset for local testing\n",
    "data = featureFormat(my_dataset, features_list, sort_keys = True)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "\n",
    "### Split data into training and testing datasets\n",
    "features_train, features_test, labels_train, labels_test = \\\n",
    "    train_test_split(features, labels, test_size=0.3, random_state=42)\n",
    "    \n",
    "### Use DecisionTreeClassifier for best 9 features and 'poi'\n",
    "print 'Decision Tree Classifier for best 9 features and \"poi\":'\n",
    "testClassifier(DecisionTreeClassifier(random_state=42))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see that our both precision and recall scores became equal to 0.5. It is totally improvement than if we will use all features from our dataset, so we will be using the following features which were selected using SelectKBest method:\n",
    "- poi\n",
    "- deferral_payments\n",
    "- exercised_stock_options\n",
    "- expenses\n",
    "- loan_advances\n",
    "- from_messages\n",
    "- other\n",
    "- director_fees\n",
    "- deferred_income\n",
    "- long_term_incentive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm Selection and Tunning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm Selection\n",
    "\n",
    "I chose Adaboost algorithm as a final one. I also evaluated Random Forest and K-Nearest Neighbors algorithms. I decided to choose Adaboost classifier because of its not bad precision and recall scores (0.5 each), and 2 others algorithms to tune using GridSearchCV (they had bad recall scores ~ 1.67). After the algorithm parameter tuning stage was completed the Random Forest and K-Nearest Neighbors classifiers didn't show any improvements in scores. So as a result Adaboost became the winner in this game. When the Adaboost classifier was evaluated with the provided tester.py script it obtained the following scores (accuracy: 0.86067, precision: 0.47273, recall: 0.39000).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adaboost Classifier:\n",
      "Training time:  1494759662.22\n",
      "Testing time:  1494759662.3\n",
      "Accuracy:  0.860465116279\n",
      "Precision:  0.5\n",
      "Recall:  0.5\n",
      "F1 score: 0.5\n"
     ]
    }
   ],
   "source": [
    "### Adaboost\n",
    "print 'Adaboost Classifier:'\n",
    "testClassifier(AdaBoostClassifier(random_state=42))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Classifier:\n",
      "Training time:  1494759677.62\n",
      "Testing time:  1494759677.65\n",
      "Accuracy:  0.883720930233\n",
      "Precision:  1.0\n",
      "Recall:  0.166666666667\n",
      "F1 score: 0.285714285714\n"
     ]
    }
   ],
   "source": [
    "### Random Forest\n",
    "print 'Random Forest Classifier:'\n",
    "testClassifier(RandomForestClassifier(random_state=42))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-Nearest Neighbors Classifier:\n",
      "Training time:  1494759693.13\n",
      "Testing time:  1494759693.13\n",
      "Accuracy:  0.860465116279\n",
      "Precision:  0.5\n",
      "Recall:  0.166666666667\n",
      "F1 score: 0.25\n"
     ]
    }
   ],
   "source": [
    "### K-Nearest Neighbors\n",
    "print 'K-Nearest Neighbors Classifier:'\n",
    "testClassifier(KNeighborsClassifier())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning of algorithms\n",
    "\n",
    "The parameters of machine learning algorithms can have a lot of influence on the algorithms performance on a given metric. An incorrectly tuned algorithm can give poor performance on the training set or overfitting. To tune the parameters of the Random Forest and K-Nearest Neighbors classifiers I used a cross validated grid search. Different sets of parameters for each classifier were tried:\n",
    "- splitting criterion ('gini' or 'entropy'), number of estimators (1 and 10) and methods for finding max number of features ('auto', 'sqrt', 'log2') (for Random Forest classifier) \n",
    "- number of neighbors (1 and 5), weights ('uniform', 'distance') and algorithm ('auto', 'ball_tree', 'kd_tree', 'brute') (for K-Nearest Neighbors classifier).\n",
    "\n",
    "The best combination of features and parameters per classifier is chosen to predict the POIs using the testing set.\n",
    "In comparison with the first estimation of the classifiers it can be seen that both Random Forest and K-Nearest Neighbors classifiers didn't improve their scores at all. That's why we won't select them for further analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest using GridSearchCV:\n",
      "Training time:  1494759745.13\n",
      "Testing time:  1494759745.73\n",
      "Accuracy:  0.883720930233\n",
      "Precision:  1.0\n",
      "Recall:  0.166666666667\n",
      "F1 score: 0.285714285714\n"
     ]
    }
   ],
   "source": [
    "### Random Forest using GridSearchCV\n",
    "print 'Random Forest using GridSearchCV:'\n",
    "param_grid = {\"n_estimators\": [1, 10],\n",
    "              \"criterion\": ('gini', 'entropy'),\n",
    "              \"max_features\": ('auto', 'sqrt', 'log2'),\n",
    "              \"random_state\": [42]}\n",
    "\n",
    "testClassifier(GridSearchCV(RandomForestClassifier(), param_grid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Nearest Neighbors without Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-Nearest Neighbors using GridSearchCV:\n",
      "Training time:  1494759764.23\n",
      "Testing time:  1494759764.36\n",
      "Accuracy:  0.860465116279\n",
      "Precision:  0.5\n",
      "Recall:  0.166666666667\n",
      "F1 score: 0.25\n"
     ]
    }
   ],
   "source": [
    "### K-Nearest Neighbors using GridSearchCV\n",
    "print 'K-Nearest Neighbors using GridSearchCV:'\n",
    "param_grid = {\"n_neighbors\": [1, 5],\n",
    "              \"weights\": ('uniform', 'distance'),\n",
    "              \"algorithm\": ('auto', 'ball_tree', 'kd_tree', 'brute')}\n",
    "\n",
    "testClassifier(GridSearchCV(KNeighborsClassifier(), param_grid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Nearest Neighbors with Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-Nearest Neighbors using GridSearchCV and StandardScaler:\n",
      "Training time:  1494768120.34\n",
      "Testing time:  1494768120.55\n",
      "Accuracy:  0.860465116279\n",
      "Precision:  0.5\n",
      "Recall:  0.166666666667\n",
      "F1 score: 0.25\n"
     ]
    }
   ],
   "source": [
    "### K-Nearest Neighbors using GridSearchCV and StandardScaler\n",
    "print 'K-Nearest Neighbors using GridSearchCV and StandardScaler:'\n",
    "knn = KNeighborsClassifier()\n",
    "estimators = [('scale', StandardScaler()), ('knn', knn)]\n",
    "pipeline = Pipeline(estimators)\n",
    "parameters = {'knn__n_neighbors': [1, 5],\n",
    "              'knn__weights': ('uniform', 'distance'),\n",
    "              'knn__algorithm': ('auto', 'ball_tree', 'kd_tree', 'brute')}\n",
    "\n",
    "testClassifier(GridSearchCV(pipeline, parameters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see, there is no difference between precision and recall scores when we tuned K-Nearest Neighbors classifier with and without features scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selected Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adaboost Classifier:\n",
      "Training time:  1494759780.53\n",
      "Testing time:  1494759780.62\n",
      "Accuracy:  0.860465116279\n",
      "Precision:  0.5\n",
      "Recall:  0.5\n",
      "F1 score: 0.5\n"
     ]
    }
   ],
   "source": [
    "### Adaboost\n",
    "print 'Adaboost Classifier:'\n",
    "testClassifier(AdaBoostClassifier(random_state=42))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation is the testing of a machine learning algorithm by applying it to data that was not visible during training. The purpose of this is to get an estimate of the algorithms performance. A classic mistake maybe that the model fits the training data very well with low error, but performs badly on not visible test data. This can lead to an over-estimate of the algorithms ability to generalize. I validated my analysis by using a StratifiedShuffleSplit cross validation approach (was used from given tester.py script) because the dataset is kind of skew. The data set is randomly split into training and test sets while preserving the proportions of true and false labels from the original data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=1.0, n_estimators=50, random_state=42)\n",
      "\tAccuracy: 0.86067\tPrecision: 0.47273\tRecall: 0.39000\tF1: 0.42740\tF2: 0.40415\n",
      "\tTotal predictions: 15000\tTrue positives:  780\tFalse positives:  870\tFalse negatives: 1220\tTrue negatives: 12130\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### run test_classifier function from tester.py file\n",
    "test_classifier(clf, my_dataset, features_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy score tells us that on average 86% of the examples in the test were classified correctly by final classifier Adaboost. But this is not the most appropriate metric for this project because there are too many non-POIs in the dataset, and therefore in the testing set. That's why the precision score and recall score were chosen as a better scores for our dataset. \n",
    "\n",
    "**Precision** is the ratio of true positive observations among all the population that are predicted as POI (% of correct positive predictions).\n",
    "\n",
    "**Recall** is the ratio of true positive observations among all the population that are POI (% of caught positive cases).\n",
    "\n",
    "We can see Adaboost has best precision score = 0.47 and recall score = 0.39. It means that all persons predicted as POI by Adaboost, 47% are true POIs, and 39% of true POIs are caught. Overall, the precision score is better than the recall score meaning that whenever a POI gets flagged in the test set, we can be sure that it is a real POI and not a false one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this project was to train and tune a machine learning algorithm that can predict which employees are POIs using Enron financial and email data. First, we investigated given to us dataset; then we removed outliers, not numeric features and changed NaN values to 0; after that we created new features and selected 9 best of them using SelectKBest method; finally, we used AdaBoost, Random Forest and K-Nearest Neighbors classifiers to predict POI (we tuned Random Forest and K-Nearest Neighbors classifiers to receive better precision and recall scores). Cross-validation showed that Adaboost was more accurate in predicting POI, giving the best precision and recall scores of 0.47 and 0.39 respectively (both over 0.3 as required). It seems that these metrics are not good enough, however the results are good enough considering that the dataset contained very few observations of POIs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
